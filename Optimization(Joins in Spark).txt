Optimization Technique:-

Joins in Spark
==============================================================================================================================
Initilizing SparkSession

from pyspark.sql import SparkSession
from pyspark.sql.functions import sum, avg, max, min, count
	
spark = SparkSession.builder.appName("Joins_In_Spark").getOrCreate()
==============================================================================================================================
**Joins in Spark**

Create Two Simple DataFrame on Which we will Perform Joins Operation 
-------------------------------------------------------------------------------------------

df1 = spark.createDataFrame([(1,"Ram"),(2,"Shyam"),(3,"Carry"),(4,"Nagesh")],["id","name"])

df2 = spark.createDataFrame([(1,"HR"),(2,"IT"),(5,"Sales"),(6,"Account")],["id","Dept"])
==============================================================================================================================
1. Inner Join:-

df1.join(df2, on = "id", how="inner").show()
==============================================================================================================================
2. Left Join:-

df1.join(df2, on="id", how="left").show()
==============================================================================================================================
3. Right Join:-

df1.join(df2, on = "id", how="right").show()
==============================================================================================================================
4. Full Outer Join:-

df1.join(df2, on ="id", how="outer").show()
==============================================================================================================================
5. Left Semi Join:-

df1.join(df2, on ="id", how="left_semi").show()
==============================================================================================================================
6. Left Anti Join:-

df1.join(df2, on = "id", how="left_anti").show()
==============================================================================================================================
Group By:-

df1.groupBy("name").agg(count("name").alias("name_count")).show()
==============================================================================================================================
ORDERBY WITH AGGREGATIONS AND GROUPBY:
	
df.groupBy("Department").agg(sum("Salary").alias("Total_salary")).orderBy("Total_salary", ascending=False).show()
==============================================================================================================================