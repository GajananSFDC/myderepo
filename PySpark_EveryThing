PySpark :-
1я╕ПтГг RDD (Resilient Distributed Dataset):-
тАв  RDD PySpark рдХрд╛ low-level distributed collection рд╣реИред
тАв  Immutable рд╣реЛрддрд╛ рд╣реИ (create рд╣реЛрдиреЗ рдХреЗ рдмрд╛рдж modify рдирд╣реАрдВ рд╣реЛрддрд╛)ред
тАв  Parallel processing рдХреЗ рд▓рд┐рдП optimizedред
тАв  2 рддрд░реАрдХреЗ рд╕реЗ рдмрдирд╛рдпрд╛ рдЬрд╛ рд╕рдХрддрд╛ рд╣реИ: parallelize() рдФрд░ textFile()ред
ЁЯУМ Syntax:-
# From Collection
rdd = spark.sparkContext.parallelize([1, 2, 3])

# From File
rdd = spark.sparkContext.textFile("data.txt")

ЁЯУМ Example:-
# parallelize example
data = [1, 2, 3, 4, 5]
rdd = spark.sparkContext.parallelize(data)
print(rdd.collect())

# textFile example
rdd2 = spark.sparkContext.textFile("/path/to/file.txt")
print(rdd2.first())




2я╕ПтГг Actions in RDD:-
ЁЯУМ Theory
тАв	Actions рд╡рд╣ operations рд╣реИрдВ рдЬреЛ result return рдХрд░рддреЗ рд╣реИрдВ рдпрд╛ data write рдХрд░рддреЗ рд╣реИрдВред
тАв	рдпрд╣ transformations рдХреЛ execute рдХрд░рддреЗ рд╣реИрдВред
ЁЯУМ Syntax:-
rdd.collect()
rdd.count()
rdd.take(5)
rdd.first() 
ЁЯУМ Example:-
rdd = spark.sparkContext.parallelize([10, 20, 30, 40])
print(rdd.collect())  # [10, 20, 30, 40]
print(rdd.count())    # 4
print(rdd.take(2))    # [10, 20]

3я╕ПтГг Transformations in RDD
ЁЯУМ Theory
тАв	Transformations RDD рдкрд░ operations apply рдХрд░рддреЗ рд╣реИрдВ рдФрд░ new RDD return рдХрд░рддреЗ рд╣реИрдВред
тАв	Lazy evaluation рд╣реЛрддреА рд╣реИ (рдЬрдм рддрдХ action call рдирд╣реАрдВ рдХрд░рддреЗ execute рдирд╣реАрдВ рд╣реЛрддрд╛)ред
ЁЯУМ Syntax:-
rdd.map(lambda x: x*2)
rdd.filter(lambda x: x > 10)
rdd.flatMap(lambda x: x.split())



ЁЯУМ Example:-
data = ["hello world", "spark rdd"]
rdd = spark.sparkContext.parallelize(data)
words = rdd.flatMap(lambda line: line.split())
print(words.collect())  # ['hello', 'world', 'spark', 'rdd']
  
4я╕ПтГг DataFrames
ЁЯУМ Theory
тАв	DataFrame SQL table рдЬреИрд╕рд╛ structured data format рд╣реИред
тАв	рдЗрд╕рдореЗрдВ named columns рд╣реЛрддреЗ рд╣реИрдВ рдФрд░ рдпрд╣ optimized API provide рдХрд░рддрд╛ рд╣реИред
ЁЯУМ Syntax:-
data = [("Alice", 25), ("Bob", 30)]
df = spark.createDataFrame(data, ["Name", "Age"])
df.show()
ЁЯУМ Example:-
data = [("Rahul", 5000), ("Amit", 6000)]
df = spark.createDataFrame(data, ["Name", "Salary"])
df.show()
5я╕ПтГг Joins in DataFrames
ЁЯУМ Theory
тАв	Joins рджреЛ DataFrames рдХреЛ common column (key) рдкрд░ combine рдХрд░рддреЗ рд╣реИрдВред
тАв	Types: Inner, Left, Right, Full, Crossред
ЁЯУМ Syntax:-
df1.join(df2, "id", "inner")

ЁЯУМ Example:-
df1 = spark.createDataFrame([(1, "A"), (2, "B")], ["id", "val1"])
df2 = spark.createDataFrame([(1, "X"), (3, "Y")], ["id", "val2"])
df1.join(df2, "id", "inner").show()
6я╕ПтГг Aggregations in DataFrames
ЁЯУМ Theory
тАв	Aggregations data рдХреЛ summarize рдХрд░рдиреЗ рдХреЗ рд▓рд┐рдП use рд╣реЛрддреА рд╣реИрдВ (рдЬреИрд╕реЗ SUM, COUNT, AVG)ред
тАв	groupBy() рдХреЗ рд╕рд╛рде aggregate functions apply рд╣реЛрддреЗ рд╣реИрдВред
ЁЯУМ Syntax:-
from pyspark.sql.functions import sum, avg, count
df.groupBy("column").agg(sum("col"), avg("col"))
ЁЯУМ Example:-
data = [("HR", 5000), ("IT", 6000), ("HR", 5500)]
df = spark.createDataFrame(data, ["Dept", "Salary"])
df.groupBy("Dept").agg(sum("Salary"), avg("Salary")).show()
7я╕ПтГг Window Functions
ЁЯУМ Theory
тАв	Window functions data рдХреЛ partition рдХрд░рдХреЗ operate рдХрд░рддреА рд╣реИрдВред
тАв	Row ranking, running total, lag-lead рдЬреИрд╕реА calculations possible рд╣реИрдВред
ЁЯУМ Syntax:-
from pyspark.sql.window import Window
from pyspark.sql.functions import row_number
windowSpec = Window.partitionBy("Dept").orderBy("Salary")
df.withColumn("Rank", row_number().over(windowSpec))

ЁЯУМ Example:-
data = [("HR", "A", 5000), ("HR", "B", 6000), ("IT", "C", 5500)]
df = spark.createDataFrame(data, ["Dept", "Emp", "Salary"])
windowSpec = Window.partitionBy("Dept").orderBy("Salary")
df.withColumn("Rank", row_number().over(windowSpec)).show()
8я╕ПтГг File Formats
ЁЯУМ Theory
тАв	PySpark multiple file formats read/write рдХрд░ рд╕рдХрддрд╛ рд╣реИ: CSV, JSON, Parquet, ORCред
ЁЯУМ Syntax:-
spark.read.csv("file.csv", header=True, inferSchema=True)
spark.read.json("file.json")
spark.read.parquet("file.parquet")
ЁЯУМ Example:-
df_csv = spark.read.csv("data.csv", header=True)
df_json = spark.read.json("data.json")
df_parquet = spark.read.parquet("data.parquet")
9я╕ПтГг Partitioning
ЁЯУМ Theory
тАв	Partitioning data рдХреЛ рдЫреЛрдЯреЗ chunks рдореЗрдВ divide рдХрд░рддрд╛ рд╣реИ рддрд╛рдХрд┐ parallel processing fast рд╣реЛред
ЁЯУМ Syntax:-
df.repartition(4)    # Increase partitions
df.coalesce(2)       # Reduce partitions
ЁЯУМ Example:-
df = spark.range(0, 20)
df2 = df.repartition(4)
print(df2.rdd.getNumPartitions())
ЁЯФЯ Bucketing
ЁЯУМ Theory
тАв	Bucketing data рдХреЛ fixed number of buckets рдореЗрдВ divide рдХрд░рддрд╛ рд╣реИ hash function рдХреЗ base рдкрд░ред
ЁЯУМ Syntax:-
df.write.bucketBy(4, "name").sortBy("age").saveAsTable("bucketed_table")
ЁЯУМ Example:-
df = spark.createDataFrame([("A", 25), ("B", 30)], ["name", "age"])
df.write.bucketBy(2, "name").saveAsTable("bucketed_data")
1я╕ПтГг1я╕ПтГг UDF (User Defined Function)
ЁЯУМ Theory
тАв	UDF рдХрд╛ use рддрдм рд╣реЛрддрд╛ рд╣реИ рдЬрдм PySpark рдХреЗ built-in functions рдЖрдкрдХреА requirement рдХреЛ cover рди рдХрд░реЗрдВред
тАв	UDF Python function рдХреЛ SQL function рдХреА рддрд░рд╣ use рдХрд░рдиреЗ рджреЗрддрд╛ рд╣реИред
ЁЯУМ Syntax:-
from pyspark.sql.functions import udf
from pyspark.sql.types import StringType
def my_upper(name):
    return name.upper()

upper_udf = udf(my_upper, StringType())
df.withColumn("UpperName", upper_udf(df["name"]))

ЁЯУМ Example:-
data = [("alice",), ("bob",)]
df = spark.createDataFrame(data, ["name"])
def my_upper(name):
    return name.upper()
upper_udf = udf(my_upper, StringType())
df.withColumn("UpperName", upper_udf(df["name"])).show()
1я╕ПтГг2я╕ПтГг Performance Optimization
ЁЯУМ Theory
тАв	Performance improve рдХрд░рдиреЗ рдХреЗ рд▓рд┐рдП cache, broadcast, repartition рдЬреИрд╕реЗ techniques use рдХрд░рддреЗ рд╣реИрдВред
ЁЯУМ Syntax:-
df.cache()
from pyspark.sql.functions import broadcast
df1.join(broadcast(df2), "id")
ЁЯУМ Example:-
df.cache()
df.show()
from pyspark.sql.functions import broadcast
df1.join(broadcast(df2), "id").show()
1я╕ПтГг3я╕ПтГг Cache & Persist
ЁЯУМ Theory
тАв	Cache memory рдореЗрдВ data store рдХрд░рддрд╛ рд╣реИред
тАв	Persist memory + disk рджреЛрдиреЛрдВ рдореЗрдВ data store рдХрд░ рд╕рдХрддрд╛ рд╣реИред
ЁЯУМ Syntax:-
df.cache()
from pyspark import StorageLevel
df.persist(StorageLevel.MEMORY_AND_DISK)

ЁЯУМ Example:-
df.cache()
df.show()
df.persist(StorageLevel.MEMORY_AND_DISK)
df.show()
1я╕ПтГг4я╕ПтГг Repartition & Coalesce
ЁЯУМ Theory
тАв	repartition() partitions increase/decrease рдХрд░рддрд╛ рд╣реИ (shuffle рдХрд░рддрд╛ рд╣реИ)ред
тАв	coalesce() partitions decrease рдХрд░рддрд╛ рд╣реИ (shuffle рдирд╣реАрдВ рдХрд░рддрд╛)ред
ЁЯУМ Syntax:-
df.repartition(6)  
df.coalesce(2)
ЁЯУМ Example:-
df = spark.range(0, 20)
df_repart = df.repartition(6)
df_coal = df.coalesce(2)
1я╕ПтГг5я╕ПтГг Broadcast Join
ЁЯУМ Theory
тАв	Broadcast join small dataset рдХреЛ рд╕рднреА nodes рдореЗрдВ copy рдХрд░рддрд╛ рд╣реИ рдЬрд┐рд╕рд╕реЗ large dataset рдХреЗ рд╕рд╛рде join fast рд╣реЛрддрд╛ рд╣реИред
ЁЯУМ Syntax:-
from pyspark.sql.functions import broadcast
df1.join(broadcast(df2), "id")
ЁЯУМ Example:-
small_df = spark.createDataFrame([(1, "A")], ["id", "val"])
large_df = spark.range(0, 1000).withColumnRenamed("id", "id")

large_df.join(broadcast(small_df), "id").show()
1я╕ПтГг6я╕ПтГг Error Handling in PySpark
ЁЯУМ Theory
тАв	PySpark jobs рдореЗрдВ runtime errors (schema mismatch, file not found, data type errors) рдЖ рд╕рдХрддреЗ рд╣реИрдВред
тАв	Error handling рд╕реЗ program crash рд╣реЛрдиреЗ рд╕реЗ рдмрдЪрддрд╛ рд╣реИ рдФрд░ controlled messages рджрд┐рдЦрд╛рдП рдЬрд╛ рд╕рдХрддреЗ рд╣реИрдВред
ЁЯУМ Syntax:-
try:
    df = spark.read.csv("data.csv", header=True)
    df.show()
except Exception as e:
    print("Error occurred:", e)
ЁЯУМ Example:-
try:
    df = spark.read.csv("/invalid/path/file.csv", header=True)
    df.show()
except Exception as e:
    print("File read error:", e)
1я╕ПтГг7я╕ПтГг Schema Evolution
ЁЯУМ Theory
тАв	Schema evolution рд╕реЗ files рдореЗрдВ рдирдП columns add/remove рд╣реЛрдиреЗ рдкрд░ рднреА read possible рд╣реИред
тАв	Parquet рдФрд░ ORC formats рдореЗрдВ use рд╣реЛрддрд╛ рд╣реИред
ЁЯУМ Syntax:-
spark.read.option("mergeSchema", "true").parquet("/path")

ЁЯУМ Example:-
df = spark.read.option("mergeSchema", "true").parquet("/data/year=2024")
df.show()
1я╕ПтГг8я╕ПтГг Deployment & Job Scheduling
ЁЯУМ Theory
тАв	Deployment рдХрд╛ рдорддрд▓рдм PySpark job рдХреЛ production environment рдореЗрдВ рдЪрд▓рд╛рдирд╛ред
тАв	Scheduling рдХрд╛ рдорддрд▓рдм job рдХреЛ regular interval (daily, weekly) рдореЗрдВ run рдХрд░рдирд╛ред
ЁЯУМ Syntax:-
# Local / cluster submit
spark-submit --master yarn job.py
# Airflow DAG (schedule job)

ЁЯУМ Example:-
# Airflow example (pseudo)
from airflow import DAG
from airflow.operators.bash import BashOperator

dag = DAG('pyspark_job', schedule_interval='@daily')

task = BashOperator(
    task_id='run_spark',
    bash_command='spark-submit --master yarn /path/job.py',
    dag=dag
)


Question & Answer
1.	CSV file read рдХрд░рдХреЗ RDD рдмрдирд╛рдирд╛ред
2.	Server log files process рдХрд░рдирд╛ред
3.	IoT sensor data parallel load рдХрд░рдирд╛ред
4.	Large dataset filtering RDD level рдкрд░ рдХрд░рдирд╛ред
5.	Text analytics (word count) рдХрд░рдирд╛ред
6.	тАв  Sales data count рдирд┐рдХрд╛рд▓рдирд╛ред
7.	тАв  IoT device readings рдХрд╛ total records рдирд┐рдХрд╛рд▓рдирд╛ред
8.	тАв  Top 5 highest sales records рдирд┐рдХрд╛рд▓рдирд╛ред
9.	тАв  Customer feedback dataset рдХрд╛ first record рджреЗрдЦрдирд╛ред
10.	тАв  Final output рдХреЛ collect рдХрд░рдХреЗ Python processing рдореЗрдВ use рдХрд░рдирд╛ред
11.	тАв  Sales price рдХреЛ discount рдХреЗ рд╕рд╛рде map рдХрд░рдирд╛ред
12.	тАв  Negative readings рдХреЛ filter рдХрд░рдирд╛ред
13.	тАв  Text files рд╕реЗ words рдирд┐рдХрд╛рд▓рдирд╛ред
14.	тАв  IoT sensor readings рдХреЛ transformations рд╕реЗ clean рдХрд░рдирд╛ред
15.	тАв  Customer feedback рд╕реЗ unwanted characters remove рдХрд░рдирд╛ред
16.	CSV sales data рдХреЛ DataFrame рдореЗрдВ load рдХрд░рдирд╛ред
17.	HR employee records analyze рдХрд░рдирд╛ред
18.	IoT readings рдХрд╛ structured report рдмрдирд╛рдирд╛ред
19.	E-commerce orders data query рдХрд░рдирд╛ред
20.	SQL рдЬреИрд╕реА aggregations perform рдХрд░рдирд╛ред
21.	тАв  Customer table рдФрд░ Orders table join рдХрд░рдирд╛ред
22.	тАв  Product table рдФрд░ Sales table join рдХрд░рдирд╛ред
23.	тАв  IoT device info рдФрд░ readings join рдХрд░рдирд╛ред
24.	тАв  HR employee рдФрд░ department data join рдХрд░рдирд╛ред
25.	тАв  Campaign рдФрд░ lead data merge рдХрд░рдирд╛ред
26.	Department-wise employee salaries рдХрд╛ sumред
27.	Product category-wise total salesред
28.	Region-wise average revenueред
29.	Month-wise customer countред
30.	IoT device readings рдХрд╛ daily averageред
31.	тАв  Department-wise top 3 salariesред
32.	тАв  Product category-wise best-selling productsред
33.	тАв  Daily running total of salesред
34.	тАв  IoT readings рдХрд╛ moving averageред
35.	тАв  Customer transaction rankingред
36.	тАв  CSV рдореЗрдВ sales data load рдХрд░рдирд╛ред
37.	тАв  JSON logs parse рдХрд░рдирд╛ред
38.	тАв  Parquet format рдореЗрдВ big data store рдХрд░рдирд╛ред
39.	тАв  ORC format рдореЗрдВ financial data save рдХрд░рдирд╛ред
40.	тАв  Multiple formats combine рдХрд░рдХреЗ reporting рдХрд░рдирд╛ред
41.	тАв  Large dataset рдХреЛ 10 partitions рдореЗрдВ split рдХрд░рдирд╛ред
42.	тАв  Output рдХреЛ month-wise partition рдХрд░рдирд╛ред
43.	тАв  Data рдХреЛ region-wise partition рдХрд░рдирд╛ред
44.	тАв  IoT readings рдХреЛ device_id рдкрд░ partition рдХрд░рдирд╛ред
45.	тАв  Cloud storage рдкрд░ optimized writesред
46.	Customer data рдХреЛ id-based buckets рдореЗрдВ divide рдХрд░рдирд╛ред
47.	Product catalog рдХреЛ category hash buckets рдореЗрдВ рд░рдЦрдирд╛ред
48.	Transaction history рдХреЛ fast join рдХреЗ рд▓рд┐рдП bucket рдХрд░рдирд╛ред
49.	IoT readings рдХреЛ sensor_id рдХреЗ buckets рдореЗрдВ рд░рдЦрдирд╛ред
50.	Analytics queries speed up рдХрд░рдирд╛ред
51.	тАв  Customer names рдХреЛ uppercase рдХрд░рдирд╛ред
52.	тАв  Product descriptions рдХреЛ clean рдХрд░рдирд╛ред
53.	тАв  IoT readings рдХреЛ custom transformation apply рдХрд░рдирд╛ред
54.	тАв  Mobile numbers рдХреЛ format рдХрд░рдирд╛ред
55.	тАв  Text processing (sentiment, keyword extraction)ред
56.	тАв  рдмрдбрд╝реЗ datasets рдХреЛ cache рдХрд░рдХреЗ multiple queries run рдХрд░рдирд╛ред
57.	тАв  Small lookup tables рдХреЛ broadcast join рдХрд░рдирд╛ред
58.	тАв  Skewed data рдХреЛ repartition рдХрд░рдирд╛ред
59.	тАв  Query optimization for dashboard refreshред
60.	тАв  Batch jobs speed improve рдХрд░рдирд╛ред
61.	тАв  Machine learning pipeline рдореЗрдВ intermediate results cache рдХрд░рдирд╛ред
62.	тАв  Repeated transformations cache рдХрд░рдирд╛ред
63.	тАв  Heavy aggregations рдХреЗ рдмрд╛рдж result persist рдХрд░рдирд╛ред
64.	тАв  Streaming queries рдХреЗ рд▓рд┐рдП data cache рдХрд░рдирд╛ред
65.	тАв  Large ETL jobs optimize рдХрд░рдирд╛ред
66.	тАв  Output рдХреЛ evenly distribute рдХрд░рдирд╛ред
67.	тАв  Writes рдХреЛ optimized size рдореЗрдВ рдХрд░рдирд╛ред
68.	тАв  Skewed data рдХреЛ balance рдХрд░рдирд╛ред
69.	тАв  Multiple partitions рдХреЛ reduce рдХрд░рдХреЗ job speed рдмрдврд╝рд╛рдирд╛ред
70.	тАв  Aggregation рдХреЗ рдмрд╛рдж shuffle minimize рдХрд░рдирд╛ред
71.	тАв  Product master table рдХреЛ sales table рд╕реЗ join рдХрд░рдирд╛ред
72.	тАв  Currency exchange rates join рдХрд░рдирд╛ред
73.	тАв  Region lookup table join рдХрд░рдирд╛ред
74.	тАв  Small config tables join рдХрд░рдирд╛ред
75.	тАв  ML model parameters join рдХрд░рдирд╛ред
76.	Missing file path рдкрд░ error handle рдХрд░рдирд╛ред
77.	Schema mismatch handle рдХрд░рдирд╛ред
78.	Null values processing рдХреЗ рджреМрд░рд╛рди error catch рдХрд░рдирд╛ред
79.	Transformation failures log рдХрд░рдирд╛ред
80.	Large job failure рдкрд░ retry logic рд▓рдЧрд╛рдирд╛ред
81.	тАв  Year-wise parquet files merge рдХрд░рдирд╛ред
82.	тАв  Monthly sales files рдореЗрдВ рдирдП columns add рдХрд░рдирд╛ред
83.	тАв  IoT sensor readings schema change handle рдХрд░рдирд╛ред
84.	тАв  Slowly changing dimensions read рдХрд░рдирд╛ред
85.	тАв  Cloud data lake schema updates handle рдХрд░рдирд╛ред
86.	Daily sales ETL pipeline run рдХрд░рдирд╛ред
87.	IoT readings batch processing schedule рдХрд░рдирд╛ред
88.	Weekly reporting jobs deploy рдХрд░рдирд╛ред
89.	Data cleaning jobs automatic run рдХрд░рдирд╛ред
90.	ML model retraining schedule рдХрд░рдирд╛ред
